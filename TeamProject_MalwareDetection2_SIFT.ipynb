{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/park1NG/BigDataSecurity/blob/main/TeamProject_MalwareDetection2_SIFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GYJTEP-pw1T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 드라이브 설정"
      ],
      "metadata": {
        "id": "Aale8Jxv8bhY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM_o_ZM66OcU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/Shareddrives/BigDataSecurity'"
      ],
      "metadata": {
        "id": "YsYarYqG6S4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "HzfOCDO86Wxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모듈 불러오기"
      ],
      "metadata": {
        "id": "LlbI6roy8goa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "import torch.utils.data\n",
        "import os\n",
        "import time\n",
        "# 전처리\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ckUwJYuE6Zep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "from PIL import Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True # prevent truncate error"
      ],
      "metadata": {
        "id": "3LEOq0sGlUdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 불러오기\n"
      ],
      "metadata": {
        "id": "UMfcc6IX8l-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, malware_list, transform=None):\n",
        "        self.root = root\n",
        "        self.malware_list = malware_list\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "        self.folder_to_label = {}  # 폴더명과 클래스 레이블 매핑을 위한 딕셔너리\n",
        "        self.class_counts = {}  # 클래스별 이미지 개수를 저장하는 딕셔너리\n",
        "\n",
        "        for idx, folder_name in enumerate(os.listdir(root)):\n",
        "            if folder_name in malware_list:\n",
        "                folder_path = os.path.join(root, folder_name)\n",
        "                self.folder_to_label[folder_name] = self.label_transform(folder_name)  # 폴더명에 대한 레이블 변환 결과 저장\n",
        "\n",
        "                count = 0  # 클래스별 이미지 개수 초기화\n",
        "                for image_name in os.listdir(folder_path):\n",
        "                    if image_name.endswith('.png'):  # 이미지 파일 확장자 지정\n",
        "                        image_path = os.path.join(folder_path, image_name)\n",
        "                        self.data.append(image_path)\n",
        "                        self.targets.append(folder_name)\n",
        "                        count += 1  # 클래스별 이미지 개수 증가\n",
        "\n",
        "                self.class_counts[folder_name] = count\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.data[index]\n",
        "        target = self.targets[index]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        target = self.folder_to_label[target]  # 폴더명에 대한 레이블 변환 결과 가져오기\n",
        "        return image, target, image_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def label_transform(self, label):\n",
        "        # 예시: 폴더명을 기준으로 클래스 레이블 변환\n",
        "        if label != 'Other':\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "def concat_datasets(datasets):\n",
        "    return ConcatDataset(datasets)\n"
      ],
      "metadata": {
        "id": "nMHaAty5DaVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지의 RGB 채널별 통계량 확인 함수\n",
        "def print_stats(dataset):\n",
        "    imgs = np.array([img.numpy() for img, _, _ in dataset])\n",
        "    print(f'shape: {imgs.shape}')\n",
        "    \n",
        "    min_r = np.min(imgs, axis=(2, 3))[:, 0].min()\n",
        "    min_g = np.min(imgs, axis=(2, 3))[:, 1].min()\n",
        "    min_b = np.min(imgs, axis=(2, 3))[:, 2].min()\n",
        "\n",
        "    max_r = np.max(imgs, axis=(2, 3))[:, 0].max()\n",
        "    max_g = np.max(imgs, axis=(2, 3))[:, 1].max()\n",
        "    max_b = np.max(imgs, axis=(2, 3))[:, 2].max()\n",
        "\n",
        "    mean_r = np.mean(imgs, axis=(2, 3))[:, 0].mean()\n",
        "    mean_g = np.mean(imgs, axis=(2, 3))[:, 1].mean()\n",
        "    mean_b = np.mean(imgs, axis=(2, 3))[:, 2].mean()\n",
        "\n",
        "    std_r = np.std(imgs, axis=(2, 3))[:, 0].std()\n",
        "    std_g = np.std(imgs, axis=(2, 3))[:, 1].std()\n",
        "    std_b = np.std(imgs, axis=(2, 3))[:, 2].std()\n",
        "    \n",
        "    print(f'min: {min_r, min_g, min_b}')\n",
        "    print(f'max: {max_r, max_g, max_b}')\n",
        "    print(f'mean: {mean_r, mean_g, mean_b}')\n",
        "    print(f'std: {std_r, std_g, std_b}')"
      ],
      "metadata": {
        "id": "w-xpldr7X8_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),        \n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "L5CiSjeYYFMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_transforms1 = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),        \n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}\n",
        "\n",
        "image_transforms2 = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),        \n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "yE3cKQrFlhK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "malware_list = {'Adposhel': 0,\n",
        " 'Allaple': 0,\n",
        " 'Amonetize': 0,\n",
        " 'Autorun': 0,\n",
        " 'Other': 1,\n",
        " 'BrowseFox': 0,\n",
        " 'Dinwod': 0,\n",
        " 'InstallCore': 0,\n",
        " 'MultiPlug': 0,\n",
        " 'VBA': 0,\n",
        " 'Vilsel': 0}"
      ],
      "metadata": {
        "id": "wlneD9fy4ivH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_transform(label):\n",
        "    # 예시: 클래스 레이블을 원하는 형태로 변환\n",
        "    if label != 4:\n",
        "      return 0\n",
        "    else:\n",
        "      return 1"
      ],
      "metadata": {
        "id": "Lax6SL6-4l11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path1 = './malware/train'\n",
        "test_path1 = './malware/val'\n",
        "train_path2 = './malware2/train'\n",
        "test_path2 = './malware2/val'"
      ],
      "metadata": {
        "id": "zJGDdX9NDeTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data1 = CustomDataset(root = train_path1, malware_list=malware_list,\n",
        "                                  transform = pre_transforms['train'])\n",
        "train_data2 = CustomDataset(root = train_path2, malware_list=malware_list,\n",
        "                                  transform = pre_transforms['train'])\n",
        "\n",
        "test_data1 = CustomDataset(root = test_path1, malware_list=malware_list,\n",
        "                                  transform = pre_transforms['test'])\n",
        "test_data2 = CustomDataset(root = test_path2, malware_list=malware_list,\n",
        "                                  transform = pre_transforms['test'])"
      ],
      "metadata": {
        "id": "y40ZwosJlzy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset rebalance \n",
        "test_data, add_data = torch.utils.data.random_split(test_data2, [2714, 220])\n",
        "\n",
        "train_data = ConcatDataset([train_data1, train_data2, test_data1, add_data])"
      ],
      "metadata": {
        "id": "EqnvZoWyYthB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_stats(train_data)"
      ],
      "metadata": {
        "id": "n0WacgcXY2vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_stats(test_data)"
      ],
      "metadata": {
        "id": "GJ77_jJ9Y2th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data1 = CustomDataset(root = train_path1, malware_list=malware_list,\n",
        "                                  transform = image_transforms1['train'])\n",
        "train_data2 = CustomDataset(root = train_path2, malware_list=malware_list,\n",
        "                                  transform = image_transforms2['train'])\n",
        "\n",
        "test_data1 = CustomDataset(root = test_path1, malware_list=malware_list,\n",
        "                                  transform = image_transforms1['test'])\n",
        "test_data2 = CustomDataset(root = test_path2, malware_list=malware_list,\n",
        "                                  transform = image_transforms2['test'])"
      ],
      "metadata": {
        "id": "SfHDr1YE8r4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total : 13,566\n",
        "# Train : 10852\n",
        "# Test : 2714(1357 * 2)"
      ],
      "metadata": {
        "id": "uE6FNq_pxa9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data1))\n",
        "print(len(train_data2))\n",
        "print(len(test_data1))\n",
        "print(len(test_data2))"
      ],
      "metadata": {
        "id": "r4RD3L18e5fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset rebalance \n",
        "test_data, add_data = torch.utils.data.random_split(test_data2, [2714, 220])\n",
        "\n",
        "train_data = ConcatDataset([train_data1, train_data2, test_data1, add_data])"
      ],
      "metadata": {
        "id": "IbvHCg1jk2u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TestLoader(Dataset):\n",
        "  val_size = int(0.5 * len(Dataset))\n",
        "  test_size = int(0.5 * len(Dataset))\n",
        "\n",
        "  valid_data, test_data = torch.utils.data.random_split(Dataset, [val_size, test_size])\n",
        "\n",
        "  valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=128, shuffle=False) # make test loader\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False) # make test loader\n",
        "\n",
        "  return valid_loader, test_loader"
      ],
      "metadata": {
        "id": "L18Aa_1PDnIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True) # make train loader\n",
        "valid_loader, test_loader = TestLoader(test_data) # make valid & test loader"
      ],
      "metadata": {
        "id": "jW1vepJ_mGQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_datasize():\n",
        "  print(f'Number of training examples: {len(train_data)}')\n",
        "  print(f'Number of validation examples: {int(len(test_data) / 2)}')\n",
        "  print(f'Number of testing examples: {int(len(test_data) / 2)}')\n",
        "\n",
        "show_datasize()"
      ],
      "metadata": {
        "id": "cS7alUIxwkKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.class_to_idx  = malware_list # class name"
      ],
      "metadata": {
        "id": "mRfacWXn5Jbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = train_data.class_to_idx\n",
        "classes"
      ],
      "metadata": {
        "id": "_H9c_LG_6WKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # functions to show an image\n",
        "\n",
        "\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# # get some random training images\n",
        "# dataiter = iter(train_loader)\n",
        "# # print(dataiter.next())\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# batch_size = 16\n",
        "\n",
        "# # show images\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# # print labels\n",
        "# print()\n",
        "\n",
        "# labels = labels.tolist()\n",
        "# print(' '.join(f'{list(classes.keys())[list(classes.values()).index(j)]}' for j in labels))"
      ],
      "metadata": {
        "id": "A5r9_7v1mCHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "9kIZ0uDE8sfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laplacian Filtering (Edge 강화) && Bilateral Filtering (노이즈 제거) 적용"
      ],
      "metadata": {
        "id": "b-MA38uyZAg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Train 폴더 위치\n",
        "train_folder = './malware/train/'\n",
        "\n",
        "# Bila_Train 폴더 위치\n",
        "bila_folder = '/home/Bila_folder1'\n",
        "\n",
        "# Laplace_Bila_Train 폴더 위치\n",
        "both_folder = '/home/both_folder1'\n",
        "\n",
        "# 모든 클래스에 대해 불러옵니다.\n",
        "for class_folder in os.walk(train_folder).__next__()[1]:\n",
        "    # 클래스 이름을 불러옵니다.\n",
        "    class_name = class_folder\n",
        "    # 클래스 폴더의 경로를 붙여넣습니다.\n",
        "    class_folder_path = os.path.join(train_folder, class_folder) \n",
        "    # Bila_Train 폴더에 저장할 경로를 지정하고 생성합니다.\n",
        "    bila_class_folder_path = os.path.join(bila_folder, class_folder)\n",
        "    # Both_Train 폴더에 저장할 경로를 지정하고 생성합니다.\n",
        "    both_class_folder_path = os.path.join(both_folder, class_folder)\n",
        "    os.makedirs(both_class_folder_path, exist_ok=True)\n",
        "\n",
        "    # 클래스 폴더를 순회하며 Laplacian Filtering과 Bilateral Filtering을 적용한 이미지를 저장합니다.\n",
        "    for image_file in os.listdir(class_folder_path):\n",
        "        # 파일 경로를 붙여넣습니다.\n",
        "        image_file_path = os.path.join(class_folder_path, image_file)\n",
        "\n",
        "        # 이미지 파일을 불러옵니다.\n",
        "        image = cv2.imread(image_file_path)\n",
        "\n",
        "        # 이미지가 제대로 불러와지지 않은 경우 다음 이미지로 넘어갑니다.\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        # Laplacian Filtering을 적용합니다.\n",
        "        lap_image = cv2.Laplacian(image, cv2.CV_8U, ksize=3)\n",
        "\n",
        "        # Bilateral Filtering을 적용합니다.\n",
        "        bila_image = cv2.bilateralFilter(image, -1, 10, 5)\n",
        "\n",
        "        # Laplace_Bila_Train 폴더에 라플라시안 이미지를 저장합니다.\n",
        "        lap_image_path = os.path.join(both_class_folder_path, 'lap_' + image_file)\n",
        "        cv2.imwrite(lap_image_path, lap_image)\n",
        "\n",
        "        # Laplace_Bila_Train 폴더에 Bilateral 이미지를 저장합니다.\n",
        "        bila_image_path = os.path.join(both_class_folder_path, 'bila_' + image_file)\n",
        "        cv2.imwrite(bila_image_path, bila_image)\n",
        "\n",
        "        # 파일 경로를 붙여넣습니다.\n",
        "        both_image_path = os.path.join(both_class_folder_path, 'both_' + image_file)\n",
        "\n",
        "        # both 이미지가 이미 저장된 경우 lap_image와 bila_image의 기존 이미지를 제거하고 both_image를 저장합니다.\n",
        "        if os.path.isfile(both_image_path):\n",
        "            os.remove(lap_image_path)\n",
        "            os.remove(bila_image_path)\n",
        "\n",
        "        # 이미지를 불러와서 Laplacian Filtering과 Bilateral Filtering을 모두 적용합니다.\n",
        "        lap_image = cv2.imread(lap_image_path)\n",
        "        bila_image = cv2.imread(bila_image_path)\n",
        "\n",
        "        # 이미지가 제대로 불러와지지 않은 경우 다음 이미지로 넘어갑니다.\n",
        "        if lap_image is None or bila_image is None:\n",
        "            continue\n",
        "\n",
        "        both_image = cv2.addWeighted(lap_image, 0.5, bila_image, 0.5, 0)\n",
        "\n",
        "        # Laplace_Bila_Train 폴더에 Laplacian Filtering과 Bilateral Filtering을 모두 적용한 이미지를 저장합니다.\n",
        "        cv2.imwrite(both_image_path, both_image)"
      ],
      "metadata": {
        "id": "0y1-WZ0kZBRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Train 폴더 위치\n",
        "train_folder = './malware2/train/'\n",
        "\n",
        "# Bila_Train 폴더 위치\n",
        "bila_folder = '/home/Bila_folder2'\n",
        "\n",
        "# Laplace_Bila_Train 폴더 위치\n",
        "both_folder = '/home/both_folder2'\n",
        "\n",
        "# 모든 클래스에 대해 불러옵니다.\n",
        "for class_folder in os.walk(train_folder).__next__()[1]:\n",
        "    # 클래스 이름을 불러옵니다.\n",
        "    class_name = class_folder\n",
        "    # 클래스 폴더의 경로를 붙여넣습니다.\n",
        "    class_folder_path = os.path.join(train_folder, class_folder) \n",
        "    # Bila_Train 폴더에 저장할 경로를 지정하고 생성합니다.\n",
        "    bila_class_folder_path = os.path.join(bila_folder, class_folder)\n",
        "    # Both_Train 폴더에 저장할 경로를 지정하고 생성합니다.\n",
        "    both_class_folder_path = os.path.join(both_folder, class_folder)\n",
        "    os.makedirs(both_class_folder_path, exist_ok=True)\n",
        "\n",
        "    # 클래스 폴더를 순회하며 Laplacian Filtering과 Bilateral Filtering을 적용한 이미지를 저장합니다.\n",
        "    for image_file in os.listdir(class_folder_path):\n",
        "        # 파일 경로를 붙여넣습니다.\n",
        "        image_file_path = os.path.join(class_folder_path, image_file)\n",
        "\n",
        "        # 이미지 파일을 불러옵니다.\n",
        "        image = cv2.imread(image_file_path)\n",
        "\n",
        "        # 이미지가 제대로 불러와지지 않은 경우 다음 이미지로 넘어갑니다.\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        # Laplacian Filtering을 적용합니다.\n",
        "        lap_image = cv2.Laplacian(image, cv2.CV_8U, ksize=3)\n",
        "\n",
        "        # Bilateral Filtering을 적용합니다.\n",
        "        bila_image = cv2.bilateralFilter(image, -1, 10, 5)\n",
        "\n",
        "        # Laplace_Bila_Train 폴더에 라플라시안 이미지를 저장합니다.\n",
        "        lap_image_path = os.path.join(both_class_folder_path, 'lap_' + image_file)\n",
        "        cv2.imwrite(lap_image_path, lap_image)\n",
        "\n",
        "        # Laplace_Bila_Train 폴더에 Bilateral 이미지를 저장합니다.\n",
        "        bila_image_path = os.path.join(both_class_folder_path, 'bila_' + image_file)\n",
        "        cv2.imwrite(bila_image_path, bila_image)\n",
        "\n",
        "        # 파일 경로를 붙여넣습니다.\n",
        "        both_image_path = os.path.join(both_class_folder_path, 'both_' + image_file)\n",
        "\n",
        "        # both 이미지가 이미 저장된 경우 lap_image와 bila_image의 기존 이미지를 제거하고 both_image를 저장합니다.\n",
        "        if os.path.isfile(both_image_path):\n",
        "            os.remove(lap_image_path)\n",
        "            os.remove(bila_image_path)\n",
        "\n",
        "        # 이미지를 불러와서 Laplacian Filtering과 Bilateral Filtering을 모두 적용합니다.\n",
        "        lap_image = cv2.imread(lap_image_path)\n",
        "        bila_image = cv2.imread(bila_image_path)\n",
        "\n",
        "        # 이미지가 제대로 불러와지지 않은 경우 다음 이미지로 넘어갑니다.\n",
        "        if lap_image is None or bila_image is None:\n",
        "            continue\n",
        "\n",
        "        both_image = cv2.addWeighted(lap_image, 0.5, bila_image, 0.5, 0)\n",
        "\n",
        "        # Laplace_Bila_Train 폴더에 Laplacian Filtering과 Bilateral Filtering을 모두 적용한 이미지를 저장합니다.\n",
        "        cv2.imwrite(both_image_path, both_image)"
      ],
      "metadata": {
        "id": "CqpYanGji1eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HOG"
      ],
      "metadata": {
        "id": "FWmDitISpg_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hog_features = []\n",
        "hog = cv2.HOGDescriptor()"
      ],
      "metadata": {
        "id": "tWeu-VIFZF9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-image"
      ],
      "metadata": {
        "id": "mvNsGu4zZGhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.util import view_as_blocks\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "mUpKHcYAZHua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabor 필터 생성 함수\n",
        "def create_filters(scales, orientations):\n",
        "    filters = []\n",
        "    for scale in range(scales[0], scales[1] + 1):\n",
        "        for orientation in np.arange(0, np.pi, np.pi / orientations):\n",
        "            filt_real = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0, ktype=cv2.CV_32F)\n",
        "            filt_imag = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0.5 * np.pi, ktype=cv2.CV_32F)\n",
        "            filt = filt_real + filt_imag\n",
        "            filt /= 2.0 * np.pi * scale * scale\n",
        "            filters.append(filt)\n",
        "    return filters\n",
        "\n",
        "# HOG 디스크립터 계산 함수\n",
        "def hog_descriptor_single_channel(image, scales=(8, 8), orientations=8, blocks=(4, 4)):    # Gabor 필터 생성\n",
        "    filters = create_filters(scales, orientations)\n",
        "    \n",
        "    # 이미지 크기와 블록 크기 계산\n",
        "    height, width = image.shape[:2]\n",
        "    block_size = height // blocks[0], width // blocks[1]\n",
        "\n",
        "    padding_size = blocks[0] * block_size[0] - height, blocks[1] * block_size[1] - width\n",
        "    \n",
        "    # 이미지 패딩 (필요한 경우)\n",
        "    if padding_size != (0, 0):\n",
        "        image = cv2.copyMakeBorder(image, 0, padding_size[0], 0, padding_size[1], cv2.BORDER_CONSTANT, value=0)\n",
        "    \n",
        "    # 이미지를 블록으로 분할\n",
        "    block_shape = (block_size[0], block_size[1])\n",
        "    blocks = view_as_blocks(image, block_shape=(block_size[0], block_size[1])).reshape(-1, *block_size, order='F')\n",
        "    \n",
        "    # 각 블록의 GIST 특성 추출\n",
        "    features = []\n",
        "    for block in blocks:\n",
        "        feats = []\n",
        "        for scale in filters:\n",
        "            for filt in scale:\n",
        "                filtered = cv2.filter2D(block, cv2.CV_64F, filt)\n",
        "                feats.append(filtered.mean())\n",
        "        features.append(feats)\n",
        "    \n",
        "    # 전체 GIST 디스크립터로 결합\n",
        "    return np.concatenate(features)\n",
        "\n",
        "\n",
        "def hog_descriptor(image, scales=(8, 8), orientations=8, blocks=(4, 4)):\n",
        "    if len(image.shape) == 3:\n",
        "    # 각 채널에 대해 GIST 디스크립터 계산\n",
        "        descriptors = [hog_descriptor_single_channel(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), scales, orientations, blocks)]            # 전체 GIST 디스크립터로 결합\n",
        "        return np.concatenate(descriptors)\n",
        "    else:\n",
        "    # 단일 채널 이미지의 경우 GIST 디스크립터를 한 번만 계산\n",
        "        return hog_descriptor_single_channel(image, scales, orientations, blocks)"
      ],
      "metadata": {
        "id": "OMbZDXGqZJGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 이미지에 맞는 레이블 생성\n",
        "def load_labels(dir, num_samples):\n",
        "    y = []\n",
        "    for subdir in sorted(os.listdir(dir)):\n",
        "        subdir_path = os.path.join(dir, subdir)\n",
        "        if os.path.isdir(subdir_path):\n",
        "            for i, filename in enumerate(sorted(os.listdir(subdir_path))):\n",
        "                y.append(labels_dict[subdir])\n",
        "                if len(y) == num_samples:\n",
        "                    break\n",
        "        if len(y) == num_samples:\n",
        "            break\n",
        "    return np.array(y)"
      ],
      "metadata": {
        "id": "-Ay2mEfnvQAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 악성코드 이미지 폴더에서 350개의 이미지에 대한 gist descriptor를 계산하여 반환\n",
        "def get_hog_descriptors(train_data):\n",
        "    descriptors = []\n",
        "    labels = []\n",
        "    for i, v in enumerate(train_data):\n",
        "      _,label,path = v\n",
        "      # 파일 경로 생성\n",
        "      # 이미지 로드\n",
        "      image = cv2.imread(path)\n",
        "      # 이미지에 대한 GIST 디스크립터 계산\n",
        "      descriptor = hog_descriptor(image)\n",
        "      descriptors.append(descriptor)\n",
        "      labels.append(label)\n",
        "      if i % 100 == 99:\n",
        "        print(\"\\tProcessed\", i + 1, \"images\")\n",
        "    return np.array(descriptors) , np.array(labels)"
      ],
      "metadata": {
        "id": "7tExoHv8ZKr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptors, train_labels = get_hog_descriptors(train_data)\n",
        "print(len(descriptors))\n",
        "print('HOG Descriptor Shape:', descriptors.shape)"
      ],
      "metadata": {
        "id": "VM3RYA1FZL-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn xgboost"
      ],
      "metadata": {
        "id": "FnIBEtexZNTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터와 레이블 설정\n",
        "X = descriptors\n",
        "hog_features = X\n",
        "\n",
        "# hog_descriptors 및 레이블 불러오기\n",
        "X_train = X\n",
        "X_val , val_label= get_hog_descriptors(add_data)\n",
        "X_test, test_label = get_hog_descriptors(test_data)"
      ],
      "metadata": {
        "id": "umUJ5scEZOf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "KTgqHFhSgdLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 모델 정의\n",
        "models = {\n",
        "    'Random Forest': make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100)),\n",
        "    'XGBoost': XGBClassifier(learning_rate=0.01, reg_lambda=0.1),\n",
        "    'Linear SVM': make_pipeline(StandardScaler(), LinearSVC(penalty='l2', dual=False)),\n",
        "    'SMO': make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)),\n",
        "    'J48': DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "# 성능 지표\n",
        "scores = {'accuracy': accuracy_score, 'FPR': confusion_matrix, 'precision': precision_score, 'recall': recall_score, 'f1score': f1_score}\n",
        "\n",
        "# 각 모델에 대해 교차 검증 및 테스트 세트에서 성능 평가\n",
        "for model_name, model_instance in models.items():\n",
        "    print(model_name)\n",
        "    model_instance.fit(X_train_scaled, train_labels)\n",
        "    y_pred = model_instance.predict(X_test_scaled)\n",
        "    for score_name, score_func in scores.items():\n",
        "        if score_name == 'FPR':\n",
        "            cm = score_func(test_label, y_pred)\n",
        "            fp = cm.sum(axis=0) - np.diag(cm)\n",
        "            tn = cm.sum() - (cm.sum(axis=1) + fp)\n",
        "            fpr = np.mean(fp / (fp + tn))\n",
        "            print(score_name, fpr)\n",
        "        elif score_name == 'accuracy':\n",
        "            print(score_name, score_func(test_label, y_pred))\n",
        "        else:\n",
        "            print(score_name, score_func(test_label, y_pred, average='weighted'))"
      ],
      "metadata": {
        "id": "gC_5KxqEZRWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GIST"
      ],
      "metadata": {
        "id": "E_Dqm17NFFTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-image"
      ],
      "metadata": {
        "id": "g9fAU5z4qpff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.util import view_as_blocks"
      ],
      "metadata": {
        "id": "qr4x57TSr8tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabor 필터 생성 함수\n",
        "def create_filters(scales, orientations):\n",
        "    filters = []\n",
        "    for scale in range(scales[0], scales[1] + 1):\n",
        "        for orientation in np.arange(0, np.pi, np.pi / orientations):\n",
        "            filt_real = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0, ktype=cv2.CV_32F)\n",
        "            filt_imag = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0.5 * np.pi, ktype=cv2.CV_32F)\n",
        "            filt = filt_real + filt_imag\n",
        "            filt /= 2.0 * np.pi * scale * scale\n",
        "            filters.append(filt)\n",
        "    return filters\n",
        "\n",
        "# GIST 디스크립터 계산 함수\n",
        "def gist_descriptor_single_channel(image, scales=(8, 8), orientations=8, blocks=(4, 4)):    # Gabor 필터 생성\n",
        "    filters = create_filters(scales, orientations)\n",
        "    \n",
        "    # 이미지 크기와 블록 크기 계산\n",
        "    height, width = image.shape[:2]\n",
        "    block_size = height // blocks[0], width // blocks[1]\n",
        "\n",
        "    padding_size = blocks[0] * block_size[0] - height, blocks[1] * block_size[1] - width\n",
        "    \n",
        "    # 이미지 패딩 (필요한 경우)\n",
        "    if padding_size != (0, 0):\n",
        "        image = cv2.copyMakeBorder(image, 0, padding_size[0], 0, padding_size[1], cv2.BORDER_CONSTANT, value=0)\n",
        "    \n",
        "    # 이미지를 블록으로 분할\n",
        "    block_shape = (block_size[0], block_size[1])\n",
        "    blocks = view_as_blocks(image, block_shape=(block_size[0], block_size[1])).reshape(-1, *block_size, order='F')\n",
        "    \n",
        "    # 각 블록의 GIST 특성 추출\n",
        "    features = []\n",
        "    for block in blocks:\n",
        "        feats = []\n",
        "        for scale in filters:\n",
        "            for filt in scale:\n",
        "                filtered = cv2.filter2D(block, cv2.CV_64F, filt)\n",
        "                feats.append(filtered.mean())\n",
        "        features.append(feats)\n",
        "    \n",
        "    # 전체 GIST 디스크립터로 결합\n",
        "    return np.concatenate(features)\n",
        "\n",
        "\n",
        "def gist_descriptor(image, scales=(8, 8), orientations=8, blocks=(4, 4)):\n",
        "    if len(image.shape) == 3:\n",
        "    # 각 채널에 대해 GIST 디스크립터 계산\n",
        "        descriptors = [gist_descriptor_single_channel(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), scales, orientations, blocks)]            # 전체 GIST 디스크립터로 결합\n",
        "        return np.concatenate(descriptors)\n",
        "    else:\n",
        "    # 단일 채널 이미지의 경우 GIST 디스크립터를 한 번만 계산\n",
        "        return gist_descriptor_single_channel(image, scales, orientations, blocks)"
      ],
      "metadata": {
        "id": "K6l1xtUD8ysZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd "
      ],
      "metadata": {
        "id": "9SI1S6TDY9TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 악성코드 이미지 폴더에서 350개의 이미지에 대한 gist descriptor를 계산하여 반환\n",
        "def get_gist_descriptors(root_dir):\n",
        "    descriptors = []\n",
        "    for subdir in sorted(os.listdir(root_dir)):\n",
        "        subdir_path = os.path.join(root_dir, subdir)\n",
        "        if os.path.isdir(subdir_path):\n",
        "            print(\"Processing directory:\", subdir_path)\n",
        "            for i, filename in enumerate(os.listdir(subdir_path)):\n",
        "                # 파일 경로 생성\n",
        "                filepath = os.path.join(subdir_path, filename)\n",
        "                # 이미지 로드\n",
        "                image = cv2.imread(filepath)\n",
        "                # 이미지에 대한 GIST 디스크립터 계산\n",
        "                descriptor = gist_descriptor(image)\n",
        "                descriptors.append(descriptor)\n",
        "                \n",
        "                if i % 10 == 9:\n",
        "                    print(\"\\tProcessed\", i + 1, \"images\")\n",
        "    return np.array(descriptors)\n"
      ],
      "metadata": {
        "id": "6N9yeZsxb3kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"Train/\"\n",
        "# 각 폴더에서 350개의 이미지에 대한 gist descriptor 계산\n",
        "descriptors = get_gist_descriptors(root_dir)\n",
        "print(len(descriptors))\n",
        "print('GIST Descriptor Shape:', descriptors.shape)"
      ],
      "metadata": {
        "id": "SmteEYnvdI2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GIST_descriptors = descriptors"
      ],
      "metadata": {
        "id": "mqiMDLZunByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SIFT"
      ],
      "metadata": {
        "id": "LPbFf12KjLrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sift_features = []\n",
        "sift = cv2.xfeatures2d.SIFT_create()"
      ],
      "metadata": {
        "id": "XAQZhRN-CD-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabor 필터 생성 함수\n",
        "def create_filters(scales, orientations):\n",
        "    filters = []\n",
        "    for scale in range(scales[0], scales[1] + 1):\n",
        "        for orientation in np.arange(0, np.pi, np.pi / orientations):\n",
        "            filt_real = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0, ktype=cv2.CV_32F)\n",
        "            filt_imag = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0.5 * np.pi, ktype=cv2.CV_32F)\n",
        "            filt = filt_real + filt_imag\n",
        "            filt /= 2.0 * np.pi * scale * scale\n",
        "            filters.append(filt)\n",
        "    return filters\n",
        "\n",
        "# SIFT 디스크립터 계산 함수\n",
        "def sift_descriptor_single_channel(image, scales=(8, 8), orientations=8, blocks=(4, 4)):    # Gabor 필터 생성\n",
        "    filters = create_filters(scales, orientations)\n",
        "    \n",
        "    # 이미지 크기와 블록 크기 계산\n",
        "    height, width = image.shape[:2]\n",
        "    block_size = height // blocks[0], width // blocks[1]\n",
        "\n",
        "    padding_size = blocks[0] * block_size[0] - height, blocks[1] * block_size[1] - width\n",
        "    \n",
        "    # 이미지 패딩 (필요한 경우)\n",
        "    if padding_size != (0, 0):\n",
        "        image = cv2.copyMakeBorder(image, 0, padding_size[0], 0, padding_size[1], cv2.BORDER_CONSTANT, value=0)\n",
        "    \n",
        "    # 이미지를 블록으로 분할\n",
        "    block_shape = (block_size[0], block_size[1])\n",
        "    blocks = view_as_blocks(image, block_shape=(block_size[0], block_size[1])).reshape(-1, *block_size, order='F')\n",
        "    \n",
        "    # 각 블록의 GIST 특성 추출\n",
        "    features = []\n",
        "    for block in blocks:\n",
        "        feats = []\n",
        "        for scale in filters:\n",
        "            for filt in scale:\n",
        "                filtered = cv2.filter2D(block, cv2.CV_64F, filt)\n",
        "                feats.append(filtered.mean())\n",
        "        features.append(feats)\n",
        "    \n",
        "    # 전체 GIST 디스크립터로 결합\n",
        "    return np.concatenate(features)\n",
        "\n",
        "def sift_descriptor(image, scales=(8, 8), orientations=8, blocks=(4, 4)):\n",
        "    if len(image.shape) == 3:\n",
        "    # 각 채널에 대해 GIST 디스크립터 계산\n",
        "        descriptors = [sift_descriptor_single_channel(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), scales, orientations, blocks)]            # 전체 GIST 디스크립터로 결합\n",
        "        return np.concatenate(descriptors)\n",
        "    else:\n",
        "    # 단일 채널 이미지의 경우 GIST 디스크립터를 한 번만 계산\n",
        "        return sift_descriptor_single_channel(image, scales, orientations, blocks)"
      ],
      "metadata": {
        "id": "T9cnUIx2ByJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 악성코드 이미지 폴더에서 350개의 이미지에 대한 gist descriptor를 계산하여 반환\n",
        "def get_sift_descriptors(train_data):\n",
        "    descriptors = []\n",
        "    labels = []\n",
        "    for i, v in enumerate(train_data):\n",
        "      _,label,path = v\n",
        "      # 파일 경로 생성\n",
        "      # 이미지 로드\n",
        "      image = cv2.imread(path)\n",
        "      # 이미지에 대한 GIST 디스크립터 계산\n",
        "      descriptor = sift_descriptor(image)\n",
        "      descriptors.append(descriptor)\n",
        "      labels.append(label)\n",
        "      if i % 100 == 99:\n",
        "        print(\"\\tProcessed\", i + 1, \"images\")\n",
        "    return np.array(descriptors) , np.array(labels)"
      ],
      "metadata": {
        "id": "rH3EIB-oBsAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptors, train_labels = get_sift_descriptors(train_data)\n",
        "print(len(descriptors))\n",
        "print('SIFT Descriptor Shape:', descriptors.shape)"
      ],
      "metadata": {
        "id": "vOceLSv1ZXz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터와 레이블 설정\n",
        "X = descriptors\n",
        "\n",
        "# hog_descriptors 및 레이블 불러오기\n",
        "X_train = X\n",
        "X_val , val_label= get_sift_descriptors(add_data)\n",
        "X_test, test_label = get_sift_descriptors(test_data)"
      ],
      "metadata": {
        "id": "byw0Pm-PZZCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "qfGrShCBX85c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 설정\n",
        "labels_dict = classes  \n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 모델 정의\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'Linear SVM': LinearSVC(),\n",
        "    'SMO': SVC(kernel='rbf'), # SMO는 일반적으로 서포트 벡터 머신 (SVM)이 rbf 커널을 사용\n",
        "    'J48': DecisionTreeClassifier() # J48는 scikit-learn에서 Decision Tree에 해당\n",
        "}\n",
        "\n",
        "# 성능 지표\n",
        "scores = {'accuracy': accuracy_score, 'FPR': confusion_matrix, 'precision': precision_score, 'recall': recall_score, 'f1score': f1_score}\n",
        "\n",
        "# 각 모델에 대해 교차 검증 및 테스트 세트에서 성능 평가\n",
        "for model_name, model_instance in models.items():\n",
        "    print(model_name)\n",
        "    model_instance.fit(X_train_scaled, train_labels)\n",
        "    y_pred = model_instance.predict(X_test_scaled)\n",
        "    for score_name, score_func in scores.items():\n",
        "        if score_name == 'FPR':\n",
        "            cm = score_func(test_label, y_pred)\n",
        "            fp = cm.sum(axis=0) - np.diag(cm)\n",
        "            tn = cm.sum() - (cm.sum(axis=1) + fp)\n",
        "            fpr = np.mean(fp / (fp + tn))\n",
        "            print(score_name, fpr)\n",
        "        elif score_name == 'accuracy':\n",
        "            print(score_name, score_func(test_label, y_pred))\n",
        "        else:\n",
        "            print(score_name, score_func(test_label, y_pred, average='weighted'))"
      ],
      "metadata": {
        "id": "2UWXqyoMZaSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\n",
        "    ('XGBoost', xgb.XGBClassifier()),\n",
        "    ('Linear SVM', LinearSVC()),\n",
        "    ('SMO', SVC(kernel='poly', coef0=1.0, C=1.0, degree=3)),\n",
        "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
        "    ('J48', DecisionTreeClassifier(max_depth=6))\n",
        "]\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return rmse\n",
        "\n",
        "def run_cross_validation(X_train, train_labels, models, n_splits=5):\n",
        "    scores = {model_name: {'Validation Accuracy': [], 'Validation Loss': []} for model_name, _ in models}\n",
        "    \n",
        "    cv = StratifiedKFold(n_splits = n_splits)\n",
        "    for train_index, val_index in cv.split(X_train, train_labels):\n",
        "        X_cv_train, X_cv_val = X_train[train_index], X_train[val_index]\n",
        "        train_labels = np.array(train_labels)  # 이 줄을 추가하여 train_labels를 numpy 배열로 변환합니다.\n",
        "        y_cv_train, y_cv_val = train_labels[train_index], train_labels[val_index]\n",
        "    \n",
        "        for model_name, model_instance in models:\n",
        "            print(f\"Processing {model_name}...\")\n",
        "            sc = StandardScaler()\n",
        "            X_cv_train_std = sc.fit_transform(X_cv_train)\n",
        "            model = model_instance\n",
        "            model.fit(X_cv_train_std, y_cv_train)\n",
        "    \n",
        "            X_cv_val_std = sc.transform(X_cv_val)\n",
        "            y_val_pred = model.predict(X_cv_val_std)\n",
        "            scores[model_name]['Validation Accuracy'].append(accuracy_score(y_cv_val, y_val_pred))\n",
        "            scores[model_name]['Validation Loss'].append(root_mean_squared_error(y_cv_val, y_val_pred))\n",
        "    \n",
        "    return scores\n",
        "\n",
        "scores_per_model = run_cross_validation(X_train, train_labels, models)\n",
        "\n",
        "# Plot validation accuracy per model\n",
        "n_splits = 5  # or any positive integer value\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "for model_name, model_scores in scores_per_model.items():\n",
        "    plt.plot(range(1, n_splits + 1), model_scores['Validation Accuracy'], label=model_name)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation loss per model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Loss')\n",
        "for model_name, model_scores in scores_per_model.items():\n",
        "    plt.plot(range(1, n_splits + 1), model_scores['Validation Loss'], label=model_name)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jLUodbf2zht5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EDA"
      ],
      "metadata": {
        "id": "Nm3SIBpBqxeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train_data에 대한 클래스별 데이터 분포(pi graph)"
      ],
      "metadata": {
        "id": "aJ_zUt3bZje3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of images for each class\n",
        "image_counts = [train_data.class_counts[malware_name] for malware_name in classes]\n",
        "\n",
        "# Plot a pie chart\n",
        "plt.pie(image_counts, labels=classes, autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pl7dmznmZmZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###val_data에 대한 클래스별 데이터 분포(pi graph)"
      ],
      "metadata": {
        "id": "BE18IgysZpih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of images for each class in add_data\n",
        "add_data_counts = [add_data.targets.count(malware_name) for malware_name in classes]\n",
        "\n",
        "# Plot a pie chart for add_data\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(add_data_counts, labels=classes, autopct='%1.1f%%')\n",
        "plt.title(\"Class Distribution for add_data\")\n",
        "plt.show()\n",
        "\n",
        "# Count the number of images for each class in val_data\n",
        "test_data_counts = [test_data.targets.count(malware_name) for malware_name in classes]\n",
        "\n",
        "# Plot a pie chart for val_data\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(test_data_counts, labels=classes, autopct='%1.1f%%')\n",
        "plt.title(\"Class Distribution for val_data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fD8yeRWMZrCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 불러오기"
      ],
      "metadata": {
        "id": "gD8-uypZr5RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE # sklearn 사용하면 easy !! \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "UvC0K5rwkMGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시각화에 사용할 모델 불러오기"
      ],
      "metadata": {
        "id": "N0PkaiN0kT07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "model = torchvision.models.resnet18(pretrained=False)\n",
        "num_ftrs = model.fc.in_features # fc의 입력 노드 수를 산출한다. 512개\n",
        "model.fc = nn.Linear(num_ftrs, 10) # fc를 nn.Linear(num_ftrs, 10)로 대체, CIFAR10,,\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ljlf-MYokPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier 들어가기 직전에 값을 뽑아낼 것임\n",
        "# (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "print(model)"
      ],
      "metadata": {
        "id": "GnMOmvJkkSsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###T-SNE"
      ],
      "metadata": {
        "id": "V4SVucEQE6Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kkw6LgKtZhoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 데이터 T-SNE"
      ],
      "metadata": {
        "id": "5TFLcCCID9u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual = []\n",
        "deep_features = []\n",
        "\n",
        "# Define a color map for each class\n",
        "color_map = plt.cm.get_cmap('tab10', len(malware_list))\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        features = model(images)\n",
        "\n",
        "        deep_features += features.cpu().numpy().tolist()\n",
        "        actual += labels.cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "dGneXg6ulBLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=4, n_neighbors=55, min_dist=1, metric='manhattan')\n",
        "cluster = np.array(tsne.fit_transform(np.array(deep_features)))\n",
        "actual = np.array(actual)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "def color_map(index):\n",
        "    # Define your color map logic here\n",
        "    colors = ['red', 'blue']  # Example color map\n",
        "    return colors[index]\n",
        "\n",
        "for label, malware_type in malware_list.items():\n",
        "    malware_type_arr = np.array([malware_type] * len(actual))  # Convert malware_type to a NumPy array\n",
        "    idx = np.where(actual == malware_type_arr)\n",
        "    plt.scatter(cluster[idx, 0], cluster[idx, 1], marker='.', label=label, color=color_map(label))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w-rGnrc86IrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구성"
      ],
      "metadata": {
        "id": "XMOtVX1T8zu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning"
      ],
      "metadata": {
        "id": "e0yB7kJWG9oC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with GIST"
      ],
      "metadata": {
        "id": "sKlhHbHPCK_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "127llhZJ86o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터와 레이블 설정\n",
        "X = descriptors\n",
        "\n",
        "# 레이블 기록\n",
        "labels_dict = classes\n",
        "train_dir = \"Train/\"  # 이 경로에 train 데이터가 저장\n",
        "val_dir = \"Validation/\"  # 이 경로에 test 데이터가 저장\n",
        "\n",
        "# GIST_descriptors 및 레이블 불러오기\n",
        "X_train = X\n",
        "X_test = get_gist_descriptors(val_dir)\n",
        "\n",
        "# 레이블 설정\n",
        "labels_dict = classes\n",
        "\n",
        "# 각 이미지에 맞는 레이블 생성\n",
        "def load_labels(dir):\n",
        "    y = []\n",
        "    for subdir in sorted(os.listdir(dir)):\n",
        "        subdir_path = os.path.join(dir, subdir)\n",
        "        if os.path.isdir(subdir_path):\n",
        "            for i, filename in enumerate(sorted(os.listdir(subdir_path))):\n",
        "                y.append(labels_dict[subdir])\n",
        "    return np.array(y)\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 모델 정의\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'Linear SVM': LinearSVC(),\n",
        "    'SMO': SVC(kernel='rbf'), # SMO는 일반적으로 서포트 벡터 머신 (SVM)이 rbf 커널을 사용\n",
        "    'J48': DecisionTreeClassifier() # J48는 scikit-learn에서 Decision Tree에 해당\n",
        "}\n",
        "\n",
        "# 성능 지표\n",
        "scores = {'accuracy': accuracy_score, 'FPR': confusion_matrix, 'precision': precision_score, 'recall': recall_score, 'f1score': f1_score}\n",
        "\n",
        "# 각 모델에 대해 교차 검증 및 테스트 세트에서 성능 평가\n",
        "for model_name, model_instance in models.items():\n",
        "    print(model_name)\n",
        "    model_instance.fit(X_train_scaled, y_train)\n",
        "    y_pred = model_instance.predict(X_test_scaled)\n",
        "    for score_name, score_func in scores.items():\n",
        "        if score_name == 'FPR':\n",
        "            cm = score_func(y_test, y_pred)\n",
        "            fp = cm.sum(axis=0) - np.diag(cm)\n",
        "            tn = cm.sum() - (cm.sum(axis=1) + fp)\n",
        "            fpr = np.mean(fp / (fp + tn))\n",
        "            print(score_name, fpr)\n",
        "        elif score_name == 'accuracy':\n",
        "            print(score_name, score_func(y_test, y_pred))\n",
        "        else:\n",
        "            print(score_name, score_func(y_test, y_pred, average='weighted'))\n"
      ],
      "metadata": {
        "id": "L9u08FHiohj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with GIST+HOG"
      ],
      "metadata": {
        "id": "77Tl8hzqCTji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hog_gist_concatenated = np.concatenate((HOG_descriptors, GIST_descriptors), axis=1)\n",
        "hog_gist_concatenated.shape"
      ],
      "metadata": {
        "id": "WcL5lJfkCWfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 데이터와 레이블 설정\n",
        "X = hog_gist_concatenated\n",
        "\n",
        "# 레이블 기록\n",
        "labels_dict = classes\n",
        "train_dir = \"Train/\"  # 이 경로에 train 데이터가 저장\n",
        "val_dir = \"Validation/\"  # 이 경로에 test 데이터가 저장\n",
        "\n",
        "# GIST_descriptors 및 레이블 불러오기\n",
        "X_train = X\n",
        "X_test = get_gist_descriptors(val_dir)\n",
        "\n",
        "# 레이블 설정\n",
        "labels_dict = classes\n",
        "\n",
        "# 각 이미지에 맞는 레이블 생성\n",
        "def load_labels(dir):\n",
        "    y = []\n",
        "    for subdir in sorted(os.listdir(dir)):\n",
        "        subdir_path = os.path.join(dir, subdir)\n",
        "        if os.path.isdir(subdir_path):\n",
        "            for i, filename in enumerate(sorted(os.listdir(subdir_path))):\n",
        "                y.append(labels_dict[subdir])\n",
        "    return np.array(y)\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Length of X:\", len(X))\n",
        "print(\"Length of y:\", len(y))\n",
        "\n",
        "# 모델 정의\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'Linear SVM': LinearSVC(),\n",
        "    'SMO': SVC(kernel='rbf'), # SMO는 일반적으로 서포트 벡터 머신 (SVM)이 rbf 커널을 사용\n",
        "    'J48': DecisionTreeClassifier() # J48는 scikit-learn에서 Decision Tree에 해당\n",
        "}\n",
        "\n",
        "# 성능 지표\n",
        "scores = {'accuracy': accuracy_score, 'FPR': confusion_matrix, 'precision': precision_score, 'recall': recall_score, 'f1score': f1_score}\n",
        "\n",
        "# 각 모델에 대해 교차 검증 및 테스트 세트에서 성능 평가\n",
        "for model_name, model_instance in models.items():\n",
        "    print(model_name)\n",
        "    model_instance.fit(X_train_scaled, y_train)\n",
        "    y_pred = model_instance.predict(X_test_scaled)\n",
        "    for score_name, score_func in scores.items():\n",
        "        if score_name == 'FPR':\n",
        "            cm = score_func(y_test, y_pred)\n",
        "            fp = cm.sum(axis=0) - np.diag(cm)\n",
        "            tn = cm.sum() - (cm.sum(axis=1) + fp)\n",
        "            fpr = np.mean(fp / (fp + tn))\n",
        "            print(score_name, fpr)\n",
        "        elif score_name == 'accuracy':\n",
        "            print(score_name, score_func(y_test, y_pred))\n",
        "        else:\n",
        "            print(score_name, score_func(y_test, y_pred, average='weighted'))\n"
      ],
      "metadata": {
        "id": "8-hsPixdCwzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning"
      ],
      "metadata": {
        "id": "xBZt7eFTG_eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model List"
      ],
      "metadata": {
        "id": "JLjcF_rC_l-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model-DNN"
      ],
      "metadata": {
        "id": "FQOpdoLX-L6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "imgG6JLnVqtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        # 모델 구조를 정의합니다.\n",
        "        self.fc1 = nn.Linear(224 * 224 * 3, 512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # 입력 데이터를 1차원으로 평탄화합니다.\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)  # 이진 분류를 위해 sigmoid 활성화 함수를 적용합니다.\n",
        "        return x\n",
        "\n",
        "# 장치 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 입력 데이터 생성\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "\n",
        "# 모델 생성 및 출력 계산\n",
        "model = DNN().to(device)\n",
        "output = model(x)\n",
        "print(output.size())\n",
        "\n",
        "# 모델 요약 출력\n",
        "summary(model, (3, 224, 224))\n"
      ],
      "metadata": {
        "id": "84CBrtgIG72_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습"
      ],
      "metadata": {
        "id": "rX8AmKX3WxXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 평가"
      ],
      "metadata": {
        "id": "gZfRaX-NWHNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model-CNN basic"
      ],
      "metadata": {
        "id": "p_mc2Pzm-P2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        ############### Conv2d, MaxPool2d, Linear 함수에 들어갈 파라미터를 채우세요 ##############\n",
        "        self.conv1 = nn.Conv2d(3, 10, 3) # in_channel, out_channel, kernel size\n",
        "        self.pool = nn.MaxPool2d(3, 2) # kernel_size, stride\n",
        "        self.conv2 = nn.Conv2d(10, 20, 3)\n",
        "        self.fc1 = nn.Linear(56180, 160) # in_features, out_features\n",
        "        self.fc2 = nn.Linear(160, 120)\n",
        "        self.fc3 = nn.Linear(120, 1)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        ###########################################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "B1g9uChC-SSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 장치 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 입력 데이터 생성\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "\n",
        "# 모델 생성 및 출력 계산\n",
        "model = Net().to(device)\n",
        "output = model(x)\n",
        "print(output.size())\n",
        "\n",
        "# 모델 요약 출력\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "Ebd7-pex-XvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model-ResNet50"
      ],
      "metadata": {
        "id": "DmnCSZA68p1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, config, output_dim):\n",
        "        super().__init__()\n",
        "                \n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "            \n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        \n",
        "        self.layer1 = self.get_model_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_model_layer(block, n_blocks[1], channels[1], stride = 2)\n",
        "        self.layer3 = self.get_model_layer(block, n_blocks[2], channels[2], stride = 2)\n",
        "        self.layer4 = self.get_model_layer(block, n_blocks[3], channels[3], stride = 2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "        \n",
        "    def get_model_layer(self, block, n_blocks, channels, stride = 1):\n",
        "    \n",
        "        layers = []\n",
        "        \n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        \n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        \n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        x = self.sigmoid(x)\n",
        "        \n",
        "        return x, h"
      ],
      "metadata": {
        "id": "3GIVUYCI8ZHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n",
        "                               stride = stride, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
        "                               stride = 1, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
        "                             stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "                        \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "QRb75CaC8YpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    \n",
        "    expansion = 4\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
        "                               stride = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
        "                               stride = stride, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n",
        "                               stride = 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, \n",
        "                             stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "            \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "                \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "            \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "    \n",
        "        return x"
      ],
      "metadata": {
        "id": "twSUre1m8YYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### model_info"
      ],
      "metadata": {
        "id": "7tQhxT6P8viD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "Model_Info = namedtuple('Model_Info', ['block', 'n_blocks', 'channels'])\n",
        "\n",
        "Model_config = Model_Info(block = Bottleneck,\n",
        "                               n_blocks = [3, 4, 6, 3],\n",
        "                               channels = [64, 128, 256, 512])"
      ],
      "metadata": {
        "id": "3SDkZ6Nb8xO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "pretrained_model = models.resnet50(pretrained = True)\n",
        "\n",
        "# Fine Tuning\n",
        "IN_FEATURES = pretrained_model.fc.in_features \n",
        "OUTPUT_DIM = 1\n",
        "fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n",
        "pretrained_model.fc = fc\n",
        "model = CustomModel(Model_config, OUTPUT_DIM)"
      ],
      "metadata": {
        "id": "4emuVAQs8ylG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Load\n",
        "model.load_state_dict(pretrained_model.state_dict())"
      ],
      "metadata": {
        "id": "cseDfD2K8zfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "-25g1f6h80Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR_finder"
      ],
      "metadata": {
        "id": "5WjFwY-a80lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_LR = 1e-2\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=START_LR)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "3BD5cyC182aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "from torchvision import models\n",
        "\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "output = model(x)\n",
        "\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "iTxkpY_A83OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class LRFinder:\n",
        "    def __init__(self, model, optimizer, criterion, device):\n",
        "        \n",
        "        self.optimizer = optimizer\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        \n",
        "        torch.save(model.state_dict(), 'init_params.pt')\n",
        "\n",
        "    def range_test(self, iterator, end_lr = 10, num_iter = 100, \n",
        "                   smooth_f = 0.05, diverge_th = 5):\n",
        "        \n",
        "        lrs = []\n",
        "        losses = []\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "        \n",
        "        iterator = IteratorWrapper(iterator)\n",
        "        \n",
        "        for iteration in range(num_iter):\n",
        "\n",
        "            loss = self._train_batch(iterator)\n",
        "\n",
        "            #update lr\n",
        "            lr_scheduler.step()\n",
        "            \n",
        "            lrs.append(lr_scheduler.get_lr()[0])\n",
        "\n",
        "            if iteration > 0:\n",
        "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
        "                \n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "\n",
        "            losses.append(loss)\n",
        "            \n",
        "            if loss > diverge_th * best_loss:\n",
        "                print(\"Stopping early, the loss has diverged\")\n",
        "                break\n",
        "                       \n",
        "        #reset model to initial parameters\n",
        "        model.load_state_dict(torch.load('init_params.pt'))\n",
        "                    \n",
        "        return lrs, losses\n",
        "\n",
        "    def _train_batch(self, iterator):\n",
        "        \n",
        "        self.model.train()\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        x, y = iterator.get_batch()\n",
        "        \n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        \n",
        "        y_pred, _ = self.model(x)\n",
        "                \n",
        "        loss = self.criterion(y_pred, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "class ExponentialLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "class IteratorWrapper:\n",
        "    def __init__(self, iterator):\n",
        "        self.iterator = iterator\n",
        "        self._iterator = iter(iterator)\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            inputs, labels = next(self._iterator)\n",
        "        except StopIteration:\n",
        "            self._iterator = iter(self.iterator)\n",
        "            inputs, labels, *_ = next(self._iterator)\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def get_batch(self):\n",
        "        return next(self)"
      ],
      "metadata": {
        "id": "zVpRNNGX848A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "END_LR = 10\n",
        "NUM_ITER = 50\n",
        "\n",
        "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
        "lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)"
      ],
      "metadata": {
        "id": "gsyb7oK885n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lr_finder(lrs, losses, skip_start = 10, skip_end = 30):\n",
        "    \n",
        "    if skip_end == 0:\n",
        "        lrs = lrs[skip_start:]\n",
        "        losses = losses[skip_start:]\n",
        "    else:\n",
        "        lrs = lrs[skip_start:-skip_end]\n",
        "        losses = losses[skip_start:-skip_end]\n",
        "    \n",
        "    fig = plt.figure(figsize = (16,8))\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.plot(lrs, losses)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Learning rate')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True, 'both', 'x')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aGmKF75Q878P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOUND_LR = 1e-2\n",
        "\n",
        "params = [\n",
        "          {'params': model.conv1.parameters(), 'lr': FOUND_LR / 10},\n",
        "          {'params': model.bn1.parameters(), 'lr': FOUND_LR / 10},\n",
        "          {'params': model.layer1.parameters(), 'lr': FOUND_LR / 8},\n",
        "          {'params': model.layer2.parameters(), 'lr': FOUND_LR / 6},\n",
        "          {'params': model.layer3.parameters(), 'lr': FOUND_LR / 4},\n",
        "          {'params': model.layer4.parameters(), 'lr': FOUND_LR / 2},\n",
        "          {'params': model.fc.parameters()}\n",
        "         ]\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(params, lr = FOUND_LR, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)"
      ],
      "metadata": {
        "id": "2oUFzEwX88sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='.init_params.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
        "                            Default: 7\n",
        "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
        "                            Default: False\n",
        "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
        "                            Default: 0\n",
        "            path (str): checkpoint저장 경로\n",
        "                            Default: '.init_params.pt'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "TQauBjM58957"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "LmhpfKR387Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for (x, y, _) in iterator:\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        y_pred = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)  \n",
        "        acc = (y_pred.argmax(dim=1) == y).float().mean()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    epoch_loss /= len(iterator)\n",
        "    epoch_acc /= len(iterator)\n",
        "        \n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "qO-WJvKE9Aqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (x, y, _) in iterator:\n",
        "\n",
        "           x = x.to(device)\n",
        "           y = y.to(device)\n",
        "\n",
        "           y_pred = model(x)\n",
        "        \n",
        "           loss = criterion(y_pred, y)  \n",
        "           acc = (y_pred.argmax(dim=1) == y).float().mean()\n",
        "\n",
        "           epoch_loss += loss.item()\n",
        "           epoch_acc += acc.item()\n",
        "          \n",
        "    epoch_loss /= len(iterator)\n",
        "    epoch_acc /= len(iterator)\n",
        "      \n",
        "    return epoch_loss, epoch_acc_1"
      ],
      "metadata": {
        "id": "yfFjUgwB9Cjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "k6XPUIJz9DSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "result_list = []\n",
        "lr_list = []\n",
        "\n",
        "patience = 5\n",
        "\n",
        "early_stopping = EarlyStopping(patience = patience, verbose = True)\n",
        "\n",
        "for epoch in range(100):\n",
        "    \n",
        "    start_time = time.monotonic()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
        "      \n",
        "    early_stopping(valid_loss, model)\n",
        "    lr_list.append(optimizer.param_groups[0][\"lr\"]) \n",
        "\n",
        "    # patience 동안 val_loss가 감소하지 않으면 조기 종료\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "\n",
        "    # val_loss 감소하면 best model 불러오기 \n",
        "    model.load_state_dict(torch.load('.init_params.pt'))\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc_1*100:6.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc_1*100:6.2f}%')\n",
        "\n",
        "    result = {\n",
        "    'EPOCH': epoch,\n",
        "    'Train Loss': train_loss,\n",
        "    'Train acc': train_acc,\n",
        "    'Valid Loss': valid_loss,\n",
        "    'Valid acc': valid_acc}\n",
        "  \n",
        "    result_list.append(result)\n",
        "  \n",
        "result_df = pd.DataFrame(result_list)"
      ],
      "metadata": {
        "id": "5ydfXopX9D3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가"
      ],
      "metadata": {
        "id": "18G7yplz89Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss 및 Acc 변화 그래프\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(result_df['EPOCH'], result_df['Train Loss'], label='Train Loss')\n",
        "axes[0].plot(result_df['EPOCH'], result_df['Valid Loss'], label='Valid Loss')\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Loss')\n",
        "\n",
        "axes[1].plot(result_df['EPOCH'], result_df['Train acc'], label='Train acc')\n",
        "axes[1].plot(result_df['EPOCH'], result_df['Valid acc'], label='Valid acc')\n",
        "axes[1].legend()\n",
        "axes[1].set_title('ACC')\n",
        "\n",
        "# axes[2].plot(result_df['EPOCH'], result_df['Train acc_5'], label='Train acc_5')\n",
        "# axes[2].plot(result_df['EPOCH'], result_df['Valid acc_5'], label='Valid acc_5')\n",
        "# axes[2].legend()\n",
        "# axes[2].set_title('ACC_5')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IhKgwzon9zG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lr_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Learning Rate\")"
      ],
      "metadata": {
        "id": "5JihiaQO-2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('.init_params.pt'))"
      ],
      "metadata": {
        "id": "cBcW3D2388_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc @1: {test_acc*100:6.2f}%')"
      ],
      "metadata": {
        "id": "YgCrh4cL9HUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 항목별 정확도 및 컨퓨전 매트릭스 \n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt \n",
        "nb_classes = 2\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs, _ = model(inputs)\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "\n",
        "print('        Malware', '    Benign')\n",
        "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
      ],
      "metadata": {
        "id": "AgR2rehJXFDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 컨퓨전 매트릭스 heatmap 그리기\n",
        "import seaborn as sns\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Malware', 'Benign']); ax.yaxis.set_ticklabels(['Malware', 'Benign']);"
      ],
      "metadata": {
        "id": "f73SrL6vXGop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}